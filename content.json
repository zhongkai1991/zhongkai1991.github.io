{"meta":{"title":"Stat World","subtitle":"All models are wrong,but some are useful.--- Box, George E. P.","description":"Run like a HongKong journalist","author":"Kai Zhong","url":"http://zhongkai1991.github.io"},"pages":[],"posts":[{"title":"Support Vector Machine","slug":"svm","date":"2017-03-25T16:00:00.000Z","updated":"2017-03-28T23:53:05.794Z","comments":true,"path":"2017/03/26/svm/","link":"","permalink":"http://zhongkai1991.github.io/2017/03/26/svm/","excerpt":"A Support Vector Machine (SVM) can be imagined as a surface that creates a boundary between points of data plotted in multidimensional that represent examples and their feature values.","text":"A Support Vector Machine (SVM) can be imagined as a surface that creates a boundary between points of data plotted in multidimensional that represent examples and their feature values. The goal of a SVM is to create a flat boundary called a hyperplane, which divides the space to create fairly homogeneous partitions on either side. Like we see in the banner, SVM (shepherd dog) aims to find an optimal route which can perfectly divide white and black sheep. ##Preliminary Knowledge","categories":[{"name":"note","slug":"note","permalink":"http://zhongkai1991.github.io/categories/note/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://zhongkai1991.github.io/tags/Machine-Learning/"},{"name":"R","slug":"R","permalink":"http://zhongkai1991.github.io/tags/R/"}]},{"title":"HR data analysis","slug":"HR data analysis","date":"2017-03-23T23:48:16.409Z","updated":"2017-05-04T22:45:39.996Z","comments":true,"path":"2017/03/24/HR data analysis/","link":"","permalink":"http://zhongkai1991.github.io/2017/03/24/HR data analysis/","excerpt":"HR dataset aims to understand why some of the best or most experienced employees are leaving. The company want to detect the most influencial factor that can help us predict whether valuable employees will leave or not. You can download the original data onhttps://www.kaggle.com/ludobenistant/d/ludobenistant/hr-analytics/hr-analytics-1","text":"HR dataset aims to understand why some of the best or most experienced employees are leaving. The company want to detect the most influencial factor that can help us predict whether valuable employees will leave or not. You can download the original data onhttps://www.kaggle.com/ludobenistant/d/ludobenistant/hr-analytics/hr-analytics-1 Analytical procedureFirstly, we need to explain why valuable employees leave, and secondly, we want to predict who will leave next.Therefore, we propose to work with the HR department to gather relevant data about the employees and to communicate the significant effect that could explain and predict employees’ departure. Measurement and methodWe have several indexes to measure our nearly 15 000 employees’ working condition. They are satisfaction level, latest evaluation (yearly), number of project worked on, average monthly hours, time spend in the company (in years), work accident (within the past 2 years), promotion within the past 5 years, department and salary. Exploratory analysisThis dataset is from the HR department: (it doesn’t take into accountthe person that have been fired, transferred or hired in the past year.) Data cleaning stageAt this stage we want to need to check and clean data in order to improve the quality of our analysis. Data quality reportThis table describes the characteristics of each features of our ABT. Wecan see different statistical measures of central tendency andvariation. Correct variables' attibutes123456data$work_accid=as.factor(data$work_accid)data$promo_last_5yrs=as.factor(data$promo_last_5yrs)data$left_or_not=as.factor(data$left_or_not)data$department=as.factor(data$department)data$salary=as.factor(data$salary)str(data) variable 14999 obs. of 10 variables satisf_level num 0.38 0.8 0.11 0.72 0.37 0.41 0.1 0.92 0.89 0.42 … last_eval num 0.53 0.86 0.88 0.87 0.52 0.5 0.77 0.85 1 0.53 … num_proj int 2 5 7 5 2 2 6 5 5 2 … ave_mon_hrs int 157 262 272 223 159 153 247 259 224 142 … time_spend int 3 6 4 5 3 3 4 5 5 3 … work_accid Factor w/ 2 levels “0”,”1”: 1 1 1 1 1 1 1 1 1 1 … promo_last_5yrs Factor w/ 2 levels “0”,”1”: 1 1 1 1 1 1 1 1 1 1 … department Factor w/ 10 levels “accounting”,”hr”,..: 8 8 8 8 8 8 8 8 8 8 … salary Factor w/ 3 levels “high”,”low”,”medium”: 2 3 3 2 2 2 2 2 2 2 … left_or_not Factor w/ 2 levels “0”,”1”: 2 2 2 2 2 2 2 2 2 2 … {% codeblock Finding missing value %} which(is.na(data), arr.ind=TRUE) #the indices of NA values row col {% endcodeblock %} First visualisationsIn this part, we will see several plots indicating the relationship between several working conditions and the result who left.On average people who leave have a low satisfaction level, they workmore and didn’t get promoted within the past five years. Who is leaving?Let’s create a data frame with only the people that have left thecompany, so we can visualise what is the distribution of each features: #Data Exploration (Quantitative Explanotary Variables vs. Qualitative Binary Response Variable) #Making table t_bygroup=function(d, x, y, n){ # an elegant way to install a missing package if (!require(&quot;plyr&quot;)) install.packages(&quot;plyr&quot;) t &lt;- ddply(d, y, .fun = function(dd){ c(Mean = mean(dd[,x],na.rm=TRUE), Sd = sd(dd[,x],na.rm=TRUE), min=min(dd[,x]), Q1=quantile(dd[,x],0.25), Median=quantile(dd[,x],0.5), Q3=quantile(dd[,x],0.75), Max=max(dd[,x])) }) return(t) } #making histogram hist_bygroup=function(d,xx,yy,name){ if (!require(&quot;ggplot2&quot;)) install.packages(&quot;ggplot2&quot;) ggplot(d, aes_string(x=xx, color=yy, fill=yy))+ geom_histogram(aes(y=..density..), alpha=0.5, position=&quot;identity&quot;)+ geom_density(alpha=.3)+ ggtitle(name) } #making boxplot box_bygroup=function(d,xx,yy,name){ if (!require(&quot;ggplot2&quot;)) install.packages(&quot;ggplot2&quot;) ggplot(d, aes_string(x=yy, y=xx, fill=yy)) + geom_boxplot()+ ggtitle(name) } if (!require(&quot;Rmisc&quot;)) install.packages(&quot;Rmisc&quot;) # Arrangement for multiple ggplots the next step is to make a combination of both {% codeblock Histrogram and box plot %} library(Rmisc) hist_sat=hist_bygroup(d=data, xx=\"satisf_level\", yy=\"left_or_not\", \"Histogram for satisf_level\") box_sat=box_bygroup(d=data, xx=\"satisf_level\", yy=\"left_or_not\", \"Boxplot for satisf_level\") multiplot(plotlist = list(hist_sat,box_sat), cols = 2) {% endcodeblock %} Histrogram and box plot123hist_le=hist_bygroup(d=data, xx=\"last_eval\", yy=\"left_or_not\",\"Histogram for last_eval\")box_le=box_bygroup(d=data, xx=\"last_eval\", yy=\"left_or_not\",\"Boxplot for last_eval\")multiplot(plotlist = list(hist_le,box_le), cols = 2) Histrogram and box plot123hist_np=hist_bygroup(d=data, xx=\"num_proj\", yy=\"left_or_not\",\"Histogram for num_proj\")box_np=box_bygroup(d=data, xx=\"num_proj\", yy=\"left_or_not\",\"Boxplot for num_proj\")multiplot(plotlist = list(hist_np,box_np), cols = 2) 123histamh=hist_bygroup(d=data, xx=\"ave_mon_hrs\", yy=\"left_or_not\",\"Histogram for ave_mon_hrs\")boxamh=box_bygroup(d=data, xx=\"ave_mon_hrs\", yy=\"left_or_not\",\"Boxplot for ave_mon_hrs\")multiplot(plotlist = list(histamh,boxamh), cols = 2)123hist_ts=hist_bygroup(d=data, xx=\"time_spend\", yy=\"left_or_not\",\"Histogram for time_spend\")box_ts=box_bygroup(d=data, xx=\"time_spend\", yy=\"left_or_not\",\"Boxplot for time_spend\")multiplot(plotlist = list(hist_ts,box_ts), cols = 2) Data Exploration (Quantitative Explanotary Variables vs. Qualitative Binary Response Variable)12345leave_or_not &lt;- factor(data$left_or_not,labels = c(\"stay\",\"leave\"))workaccident&lt;- factor(data$work_accid,labels = c(\"never happened\", \"happened\"))promotion &lt;- factor(data$promo_last_5yrs,labels = c(\"not promoted\", \"promoted\"))library(gmodels)CrossTable(x = workaccident, y = leave_or_not, digits=3, max.width = 5, prop.r=TRUE,prop.chisq=FALSE, prop.c=FALSE,format=c(\"SPSS\")) Total Observations in Table: 14999 | work accident | stay | leave | Row Total | |—————|———–|———–|———–| |never happened | 9428 | 3402 | 12830 | | | 73.484% | 26.516% | 85.539% | | | 62.858% | 22.682% | | |—————|———–|———–|———–| | happened | 2000 | 169 | 2169 | | | 92.208% | 7.792% | 14.461% | | | 13.334% | 1.127% | | |—————|———–|———–|———–| | Column Total | 11428 | 3571 | 14999 | |—————|———–|———–|———–| CrossTable(x = promotion, y = leave_or_not, digits=3, max.width = 5, prop.r=TRUE,prop.t=TRUE,prop.chisq=FALSE, prop.c=TRUE,format=c(&quot;SPSS&quot;)) | leave_or_not | | promotion | stay | leave | Row Total | |————-|———–|———–|———–| |not promoted | 11128 | 3552 | 14680 | | | 75.804% | 24.196% | 97.873% | | | 97.375% | 99.468% | | | | 74.192% | 23.682% | | |————-|———–|———–|———–| | promoted | 300 | 19 | 319 | | | 94.044% | 5.956% | 2.127% | | | 2.625% | 0.532% | | | | 2.000% | 0.127% | | |————-|———–|———–|———–| |Column Total | 11428 | 3571 | 14999 | | | 76.192% | 23.808% | | |————-|———–|———–|———–| Here it’s much clearer. On average valuable employees that left weren’t satisfied, had higher yearly evaluation of performance, worked on many projects, spent more hours in the company each month，been less likely to experience work accident but weren’t promoted. What’s more,the variance of these attributes for those who left is larger than those who didn’t. This might indicate the company lost some outstanding and poor employees but the mediocre tend to stay. Deep InsightsHere we begin to use some machine learning methods to analyze the specific prediction problem which employee will leave given his or her working performance.Firstly we seperate the whole dataset into three parts, training data (60%),validation data(20%) and test data (20%). Stratified sampling is prefered here.To compare the performance of each method, we use Kappa statistic and prediction accuracy. Also, in order to achieve the best performance, we apply cross-validation to minimize the influence of over-fitting existed in the training data. 1.Logistic RegressionLogistic regression, as the most basic solution, will give us the first impression which attribute is significant and rough prediction which employee will leave.| predicted condition ||actual condition | 0 | 1 | Row Total ||—————–|———–|———–|———–|| 0 | 2113 | 172 | 2285 || | 70.457% | 5.735% | ||—————–|———–|———–|———–|| 1 | 472 | 242 | 714 || | 15.739% | 8.069% | ||—————–|———–|———–|———–|| Column Total | 2585 | 414 | 2999 ||—————–|———–|———–|———–|The confusion matrix and the accuracy of the logistics model show somewhat satisfactory result. Kappa is about 31%, which seems a little poor. But after all it’s a very simplemodel and give the reasonable results. 2. Decision Treectrl &lt;- trainControl(method = &quot;cv&quot;, number = 10,selectionFunction = &quot;oneSE&quot;) grid &lt;- expand.grid(.model = c(&quot;tree&quot;,&quot;rules&quot;),.trials = c(1, 5, 10, 15, 20, 25, 30, 35),.winnow =c(TRUE,FALSE)) cart&lt;- train(left_or_not ~ ., data = trainprocessdata, method = &quot;C5.0&quot;,metric = &quot;Kappa&quot;,trControl = ctrl,tuneGrid = grid) ggplot(cart) + theme_bw() We choose trial=10 and rules=false for the simple reason that this makes model simplest while at the same time doesn’t show much difference in Kappa performance from other combinations of parameter. result=C5.0(left_or_not~.,data=trainprocessdata,trials=10,rules=FALSE) predictresult=predict(result,testprocessdata) mean(predictresult==testprocessdata$left_or_not) ##get prediction accuracy=0.9836 kappa &lt;- kappa2(data.frame(testprocessdata$left_or_not, predictresult))$value kappa ##get kappa statistics=0.954371 CrossTable(testprocessdata$left_or_not,predictresult,prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c(&apos;actual condition&apos;, &apos;predicted condition&apos;),format=c(&quot;SPSS&quot;)) | predicted condition ||actual condition | 0 | 1 | Row Total ||—————–|———–|———–|———–|| 0 | 2274 | 11 | 2285 || | 75.825% | 0.367% | ||—————–|———–|———–|———–|| 1 | 38 | 676 | 714 || | 1.267% | 22.541% | ||—————–|———–|———–|———–|| Column Total | 2312 | 687 | 2999 ||—————–|———–|———–|———–| vrimp=varImp(result) vrimp[,&quot;name&quot;]=c(&quot;satisf_level&quot;,&quot;last_eval&quot;,&quot;num_proj&quot;, &quot;ave_mon_hrs&quot;,&quot;promo_last_5yrs&quot;,&quot;time_spend&quot;,&quot;salary&quot;,&quot;department&quot;,&quot;work_accid&quot;) barplot(vrimp$Overall[order(vrimp$Overall, decreasing = TRUE)],names.arg=vrimp$name,ylab=&quot;importance&quot;,ylim = c(0, 100), main = &quot;Variables Relative Importance&quot;,col = &quot;lightblue&quot;) Decision tree provides us a better method to estimate the importance of each attribute and predict employee’s action. Here we use C5.0 algorithm in that it shows superiority over other decision tree algorithem in accuracy.We can see from the variable importance histogram that satisfaction level, last evaluation, number of projects and average working hour rank the highest in deciding whether employees will leave.The confusion matrix and the accuracy of the decision tree model show much better result than logistic regression. Accuracy is about 98% and Kappa is about 95%. 3.Bagging TechniqueBagging method is a little bit complex than decision because it takes the subset of the original train dataset many times (breed many independent trees) and then rank the frequency of each attribute that appears in each tree to decide which one is more essential. set.seed(5303) bagmodel=train(left_or_not ~ ., data =trainprocessdata, method = &quot;treebag&quot;,trControl = ctrl) bagprediction=predict(bagmodel,testprocessdata) mean(bagprediction == testprocessdata$left_or_not) [1] 0.9916639 kappa &lt;- kappa2(data.frame(testprocessdata$left_or_not, bagprediction))$value kappa [1] 0.9770115 CrossTable(testprocessdata$left_or_not,bagprediction,prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c(&apos;actual condition&apos;, &apos;predicted condition&apos;),format=c(&quot;SPSS&quot;)) | | predicted condition ||actual condition | 0 | 1 | Row Total ||—————–|———–|———–|———–|| 0 | 2273 | 12 | 2285 || | 75.792% | 0.400% | ||—————–|———–|———–|———–|| 1 | 13 | 701 | 714 || | 0.433% | 23.374% | ||—————–|———–|———–|———–|| Column Total | 2286 | 713 | 2999 ||—————–|———–|———–|———–|Bagging method is almost perfect in predicting employee working condition compared with decision tree. Its accuracy reaches 99% and Kappa statistic is about 98%. 4. AdaboostBoosting technique is also a derivation of decision tree. It originates from several weak classifiers containing only the subset of all predictors. Then it gradually increases the weight of those classifiers who make the wrong prediction of output variable and trains them to be strong and accurate classifier. Adaboost is one of a powerful algorithm among boosting method. set.seed(5303) ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10,selectionFunction = &quot;oneSE&quot;) Grid &lt;- expand.grid(maxdepth=10,nu=0.1,iter=50) adamodel &lt;- train(left_or_not~., data=trainprocessdata,method = &quot;ada&quot;, trControl = ctrl,tuneGrid=Grid,metric = &quot;Accuracy&quot;,preProc = c(&quot;center&quot;, &quot;scale&quot;)) adaprediction=predict(adamodel,testprocessdata) mean(adaprediction == testprocessdata$left_or_not) [1] 0.9873291 kappa &lt;- kappa2(data.frame(testprocessdata$left_or_not, adaprediction))$value kappa [1] 0.9648031 CrossTable(testprocessdata$left_or_not,adaprediction,prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c(&apos;actual condition&apos;, &apos;predicted condition&apos;),format=c(&quot;SPSS&quot;)) | | predicted condition ||actual condition | 0 | 1 | Row Total ||—————–|———–|———–|———–|| 0 | 2274 | 11 | 2285 || | 75.825% | 0.367% | ||—————–|———–|———–|———–|| 1 | 27 | 687 | 714 || | 0.900% | 22.908% | ||—————-=|———–|———–|———–|| Column Total | 2301 | 698 | 2999 ||—————–|———–|———–|———–| The result of accuracy and Kappa proves that Adaboost performs also very well but slightly worse than bagging. 5. Random ForestRandom forest is the combination of decision tree, bagging and boosting. Like bagging, it is also the aggregate of each independent tree composed of the subset of data. Like boosting, it uses the subset of all predictors. set.seed(5303) rfranges &lt;- list(ntree = c(500, 1000, 1500), mtry = 2:6) rftune &lt;- tune(randomForest,left_or_not~ ., data =trainprocessdata, ranges = rfranges) rftune$best.parameters rfbest &lt;- rftune$best.model rfpredictions &lt;- predict(rfbest, testprocessdata) mean(rfbestpredictions==testprocessdata$left_or_not) [1] 0.9756586 kappa &lt;- kappa2(data.frame(testprocessdata$left_or_not, rfbestpredictions))$value kappa [1] 0.9323521 varImpPlot(rfbest,type=2) CrossTable(testprocessdata$left_or_not,rfpredictions,prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c(&apos;actual condition&apos;, &apos;predicted condition&apos;),format=c(&quot;SPSS&quot;)) | | predicted condition ||actual condition | 0 | 1 | Row Total ||—————–|———–|———–|———–|| 0 | 2281 | 4 | 2285 || | 76.059% | 0.133% | ||—————–|———–|———–|———–|| 1 | 13 | 701 | 714 || | 0.433% | 23.374% | ||—————–|———–|———–|———–|| Column Total | 2294 | 705 | 2999 ||—————–|———–|———–|———–| Random forest provides a good enough result (97% prediction accuracy and 93% Kappa) although it doesn’t seem as good as bagging and boosting. 6. Neural NetworkHere we applied the most widely used neural network model is the single-hidden-layer, feedforwardneural network mainly for computation convenience. set.seed(5303) ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10,selectionFunction = &quot;oneSE&quot;) nnetgrid&lt;- expand.grid(.decay = c(0.1, 0.01),.size = c(10,12,14,16,18)) nnetfit &lt;- train(left_or_not ~ ., data =trainprocessdata, method = &quot;nnet&quot;,maxit = 2000,metric = &quot;Accuracy&quot;,trControl = ctrl, tuneGrid =nnetgrid, trace = F, MaxNWts = 10000) ggplot(nnetfit) + theme_bw() size=12 and decay=0.1 seems to be an appropriate combination nnetresult=nnet(left_or_not~.,data=trainprocessdata,size=12,maxit=2000,decay=0.1) nnetpredict&lt;- predict(nnetresult,testprocessdata,type= &quot;class&quot;) mean(nnetpredict == testprocessdata$left_or_not) [1] 0.9643214 kappa &lt;- kappa2(data.frame(testprocessdata$left_or_not, nnetpredict))$value kappa [1] 0.9017038 CrossTable(testprocessdata$left_or_not,nnetpredict,prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c(&apos;actual condition&apos;, &apos;predicted condition&apos;),format=c(&quot;SPSS&quot;)) | | predicted condition ||actual condition | 0 | 1 | Row Total ||—————–|———–|———–|———–|| 0 | 2231 | 54 | 2285 || | 74.391% | 1.801% | ||—————–|———–|———–|———–|| 1 | 53 | 661 | 714 || | 1.767% | 22.041% | ||—————–|———–|———–|———–|| Column Total | 2284 | 715 | 2999 ||—————–|———–|———–|———–| Neural network’s performance is not as good as the methods as we used before except logistic regression. If we increase the number of node or hidden layer, the performance will improve at the risk of over-fitting. 7.SVMSVM provides us a conceptual framework that is quite irrelevant to the statistical learning procedure but it is actually very useful in classification problem. set.seed(5303) library(e1071) svmtune &lt;- tune(svm,left_or_not~ ., data =trainprocessdata,kernel = &quot;radial&quot;, ranges = list(cost = c(0.01, 0.1, 1, 10, 100), gamma = c(0.01, 0.05, 0.1, 0.5, 1))) svmtune svmmodel &lt;- svmtune$best.model svmpredict&lt;- predict(svmmodel,testprocessdata,type = &quot;class&quot;) mean(svmpredict==testprocessdata$left_or_not) [1] 0.9836612 kappa &lt;- kappa2(data.frame(testprocessdata$left_or_not, svmpredict))$value kappa [1] 0.9551154 CrossTable(testprocessdata$left_or_not,svmpredict,prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c(&apos;actual condition&apos;, &apos;predicted condition&apos;),format=c(&quot;SPSS&quot;)) | | predicted condition ||actual condition | 0 | 1 | Row Total ||—————–|———–|———–|———–|| 0 | 2257 | 28 | 2285 || | 75.258% | 0.934% | ||—————–|———–|———–|———–|| 1 | 21 | 693 | 714 || | 0.700% | 23.108% | ||—————–|———–|———–|———–|| Column Total | 2278 | 721 | 2999 ||—————–|———–|———–|———–| The accuracy and Kappa is approximate to the result of bagging and boosting, which shows its goodness. OverviewLast updated on the 11/2015","categories":[],"tags":[]},{"title":"wine quality data report","slug":"wine-quality-data-report","date":"2017-03-23T05:24:44.000Z","updated":"2017-03-24T22:00:41.884Z","comments":true,"path":"2017/03/23/wine-quality-data-report/","link":"","permalink":"http://zhongkai1991.github.io/2017/03/23/wine-quality-data-report/","excerpt":"","text":"","categories":[],"tags":[{"name":"data report","slug":"data-report","permalink":"http://zhongkai1991.github.io/tags/data-report/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-03-21T17:23:00.835Z","updated":"2017-03-24T06:42:41.846Z","comments":true,"path":"2017/03/22/hello-world/","link":"","permalink":"http://zhongkai1991.github.io/2017/03/22/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[{"name":"introduction","slug":"introduction","permalink":"http://zhongkai1991.github.io/tags/introduction/"}]}]}